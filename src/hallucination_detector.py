"""
Hallucination & Fact-Drift Detector for Ethics Toolkit
Detects when AI models generate false or inconsistent information
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("Warning: Transformers not available. Limited functionality.")
    TRANSFORMERS_AVAILABLE = False

class HallucinationDetector:
    """
    Detects hallucinations and factual inconsistencies in AI model outputs
    """
    
    def __init__(self, use_models=True):
        """
        Initialize the hallucination detector
        
        Args:
            use_models: Whether to load transformer models (slower but more accurate)
        """
        self.use_models = use_models and TRANSFORMERS_AVAILABLE
        self.text_generator = None
        self.fact_checker = None
        self.results_cache = {}
        
        if self.use_models:
            try:
                print("Loading hallucination detection models...")
                self.text_generator = pipeline(
                    "text2text-generation",
                    model="google/flan-t5-small"
                )
                print("âœ… Text generator loaded")
            except Exception as e:
                print(f"âš ï¸ Could not load text generator: {e}")
                self.text_generator = None
    
    def detect_hallucination_simple(self, generated_text: str, 
                                   reference_text: str) -> Dict[str, Any]:
        """
        Simple hallucination detection using text similarity
        
        Args:
            generated_text: Text generated by AI model
            reference_text: Known factual reference text
            
        Returns:
            Dictionary with detection results
        """
        # Tokenize and normalize
        gen_tokens = set(generated_text.lower().split())
        ref_tokens = set(reference_text.lower().split())
        
        # Calculate similarity metrics
        intersection = gen_tokens.intersection(ref_tokens)
        union = gen_tokens.union(ref_tokens)
        
        # Jaccard similarity
        jaccard_sim = len(intersection) / len(union) if union else 0
        
        # Overlap ratio
        overlap_ratio = len(intersection) / max(len(gen_tokens), len(ref_tokens)) if max(len(gen_tokens), len(ref_tokens)) > 0 else 0
        
        # Length difference penalty
        length_diff = abs(len(generated_text) - len(reference_text)) / max(len(generated_text), len(reference_text))
        
        # Combined score
        similarity_score = (jaccard_sim + overlap_ratio) / 2 * (1 - length_diff * 0.1)
        
        # Threshold-based detection
        hallucination_threshold = 0.3
        is_hallucination = similarity_score < hallucination_threshold
        
        return {
            'similarity_score': similarity_score,
            'jaccard_similarity': jaccard_sim,
            'overlap_ratio': overlap_ratio,
            'length_difference': length_diff,
            'is_hallucination': is_hallucination,
            'confidence': 1 - similarity_score,
            'method': 'simple_similarity'
        }
    
    def detect_consistency(self, text_list: List[str]) -> Dict[str, Any]:
        """
        Detect consistency across multiple generated texts
        
        Args:
            text_list: List of generated texts to check for consistency
            
        Returns:
            Consistency analysis results
        """
        if len(text_list) < 2:
            return {'error': 'Need at least 2 texts for consistency analysis'}
        
        similarities = []
        
        # Compare all pairs
        for i in range(len(text_list)):
            for j in range(i + 1, len(text_list)):
                result = self.detect_hallucination_simple(text_list[i], text_list[j])
                similarities.append(result['similarity_score'])
        
        # Calculate consistency metrics
        avg_consistency = np.mean(similarities)
        min_consistency = np.min(similarities)
        consistency_variance = np.var(similarities)
        
        # Flag as inconsistent if average similarity is low
        is_inconsistent = avg_consistency < 0.5
        
        return {
            'average_consistency': avg_consistency,
            'min_consistency': min_consistency,
            'consistency_variance': consistency_variance,
            'is_inconsistent': is_inconsistent,
            'pairwise_similarities': similarities,
            'num_comparisons': len(similarities)
        }
    
    def analyze_factual_drift(self, current_outputs: List[str], 
                            historical_outputs: List[str]) -> Dict[str, Any]:
        """
        Analyze factual drift between current and historical outputs
        
        Args:
            current_outputs: Recent model outputs
            historical_outputs: Historical model outputs for comparison
            
        Returns:
            Drift analysis results
        """
        if not current_outputs or not historical_outputs:
            return {'error': 'Need both current and historical outputs'}
        
        # Calculate average similarity between current and historical
        drift_scores = []
        
        for current in current_outputs:
            similarities = []
            for historical in historical_outputs:
                result = self.detect_hallucination_simple(current, historical)
                similarities.append(result['similarity_score'])
            
            # Average similarity to historical outputs
            avg_similarity = np.mean(similarities)
            drift_scores.append(1 - avg_similarity)  # Drift = 1 - similarity
        
        # Overall drift metrics
        avg_drift = np.mean(drift_scores)
        max_drift = np.max(drift_scores)
        drift_variance = np.var(drift_scores)
        
        # Flag significant drift
        drift_threshold = 0.7
        significant_drift = avg_drift > drift_threshold
        
        return {
            'average_drift': avg_drift,
            'max_drift': max_drift,
            'drift_variance': drift_variance,
            'significant_drift': significant_drift,
            'drift_threshold': drift_threshold,
            'individual_drift_scores': drift_scores
        }
    
    def generate_test_responses(self, prompts: List[str]) -> List[str]:
        """
        Generate responses for testing (if transformer models available)
        
        Args:
            prompts: List of prompts to generate responses for
            
        Returns:
            List of generated responses
        """
        if not self.use_models or not self.text_generator:
            return [f"Mock response to: {prompt}" for prompt in prompts]
        
        responses = []
        for prompt in prompts:
            try:
                response = self.text_generator(
                    prompt, 
                    max_length=100, 
                    num_return_sequences=1
                )[0]['generated_text']
                responses.append(response)
            except Exception as e:
                responses.append(f"Error generating response: {e}")
        
        return responses
    
    def comprehensive_analysis(self, prompts: List[str], 
                             reference_facts: Dict[str, str] = None) -> Dict[str, Any]:
        """
        Run comprehensive hallucination analysis
        
        Args:
            prompts: List of prompts to test
            reference_facts: Dictionary mapping prompts to known facts
            
        Returns:
            Comprehensive analysis results
        """
        print("Running comprehensive hallucination analysis...")
        
        # Generate responses
        responses = self.generate_test_responses(prompts)
        
        results = {
            'prompts': prompts,
            'responses': responses,
            'individual_analyses': [],
            'consistency_analysis': {},
            'summary': {}
        }
        
        # Individual analyses
        if reference_facts:
            for i, (prompt, response) in enumerate(zip(prompts, responses)):
                if prompt in reference_facts:
                    analysis = self.detect_hallucination_simple(
                        response, reference_facts[prompt]
                    )
                    analysis['prompt'] = prompt
                    analysis['response'] = response
                    analysis['reference'] = reference_facts[prompt]
                    results['individual_analyses'].append(analysis)
        
        # Consistency analysis
        if len(responses) > 1:
            results['consistency_analysis'] = self.detect_consistency(responses)
        
        # Summary statistics
        hallucination_count = sum(1 for a in results['individual_analyses']
                                if a['is_hallucination'])
        avg_similarity = np.mean([a['similarity_score']
                                for a in results['individual_analyses']]) if results['individual_analyses'] else 0.0
        
        results['summary'] = {
            'total_prompts': len(prompts),
            'hallucinations_detected': hallucination_count,
            'hallucination_rate': hallucination_count / len(prompts) if len(prompts) > 0 else 0.0,
            'average_similarity': avg_similarity,
            'overall_quality': 'Good' if hallucination_count == 0 else
                             'Fair' if (len(prompts) > 0 and hallucination_count / len(prompts) <= 0.3) else 'Poor'
        }
        
        # Add consistency summary to overall summary if available
        if results['consistency_analysis'] and 'error' not in results['consistency_analysis']:
            results['summary']['average_consistency'] = results['consistency_analysis']['average_consistency']
            results['summary']['is_inconsistent'] = results['consistency_analysis']['is_inconsistent']
        
        return results
    
    def generate_report(self, analysis_results: Dict[str, Any]) -> str:
        """
        Generate human-readable report from analysis results
        
        Args:
            analysis_results: Results from comprehensive_analysis
            
        Returns:
            Formatted report string
        """
        report = "# Hallucination & Fact-Drift Detection Report\n\n"
        
        # Summary
        if 'summary' in analysis_results and analysis_results['summary']:
            summary = analysis_results['summary']
            report += "## Summary\n"
            report += f"- **Total prompts analyzed**: {summary.get('total_prompts', 0)}\n"
            report += f"- **Hallucinations detected**: {summary.get('hallucinations_detected', 0)}\n"
            report += f"- **Hallucination rate**: {summary.get('hallucination_rate', 0):.1%}\n"
            report += f"- **Average similarity**: {summary.get('average_similarity', 0):.3f}\n"
            report += f"- **Overall quality**: {summary.get('overall_quality', 'Unknown')}\n\n"
        
        # Individual results
        if analysis_results.get('individual_analyses'):
            report += "## Individual Analysis\n\n"
            for i, analysis in enumerate(analysis_results['individual_analyses']):
                status = "ðŸš¨ POTENTIAL HALLUCINATION" if analysis['is_hallucination'] else "âœ… FACTUAL"
                report += f"### Prompt {i+1}: {analysis['prompt']}\n"
                report += f"**Generated**: {analysis['response']}\n\n"
                report += f"**Reference**: {analysis['reference']}\n\n"
                report += f"**Similarity Score**: {analysis['similarity_score']:.3f}\n\n"
                report += f"**Status**: {status}\n\n"
                report += "---\n\n"
        
        # Consistency analysis
        if analysis_results.get('consistency_analysis'):
            consistency = analysis_results['consistency_analysis']
            if 'error' not in consistency:
                report += "## Consistency Analysis\n"
                report += f"- **Average consistency**: {consistency['average_consistency']:.3f}\n"
                report += f"- **Minimum consistency**: {consistency['min_consistency']:.3f}\n"
                report += f"- **Is inconsistent**: {consistency['is_inconsistent']}\n\n"
        
        return report
